{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_APPLICATION_CREDENTIALS = \"/Users/tararosen/Downloads/hacker-news-79073e57c2a7.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our bq_helper package\n",
    "import bq_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a helper object for our bigquery dataset\n",
    "hacker_news = bq_helper.BigQueryHelper(active_project= \"bigquery-public-data\", \n",
    "                                       dataset_name = \"hacker_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a list of all the tables in the hacker_news dataset\n",
    "hacker_news.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print information on all the columns in the \"full\" table\n",
    "# in the hacker_news dataset\n",
    "hacker_news.table_schema(\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the first couple lines of the \"full\" table\n",
    "hacker_news.head(\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the first ten entries in the by column of the full table\n",
    "hacker_news.head(\"full\", selected_columns=\"by\", num_rows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this query looks in the full table in the hacker_news\n",
    "# dataset, then gets the score column from every row where \n",
    "# the type column has \"job\" in it.\n",
    "query = \"\"\"SELECT score\n",
    "            FROM `bigquery-public-data.hacker_news.full`\n",
    "            WHERE type = \"job\" \"\"\"\n",
    "\n",
    "# check how big this query will be\n",
    "hacker_news.estimate_query_size(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: When you're writing your query, make sure that the name of the table (next to FROM) is in back ticks (`), not single quotes ('). The reason for this is that the names of BigQuery tables contain periods in them, which in SQL are special characters. Putting the table name in back ticks protects the table name, so it's treated as a single string instead of being run as code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this query if it's less than 100 MB\n",
    "hacker_news.query_to_pandas_safe(query, max_gb_scanned=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the scores of job postings (if the \n",
    "# query is smaller than 1 gig)\n",
    "job_post_scores = hacker_news.query_to_pandas_safe(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_post_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average score for job posts\n",
    "job_post_scores['score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_post_scores.score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our dataframe as a .csv \n",
    "job_post_scores.to_csv(\"job_post_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Scavenger Hunt Day 1 - SELECT, FROM and WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a helper object for this dataset\n",
    "open_aq = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",\n",
    "                                   dataset_name=\"openaq\")\n",
    "\n",
    "# print all the tables in this dataset (there's only one!)\n",
    "open_aq.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_aq.head('global_air_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT city\n",
    "           FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "           WHERE country = 'US'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities = open_aq.query_to_pandas_safe(query, max_gb_scanned=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities['city'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the questions I would like you to get the data to answer:\n",
    "\n",
    "* Which countries use a unit other than ppm to measure any type of pollution? (Hint: to get rows where the value isn't something, use \"!=\")\n",
    "* Which pollutants have a value of exactly 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find column names\n",
    "open_aq.table_schema('global_air_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which countries use a unit other than ppm to measure any type of pollution?\n",
    "query = (\"\"\"SELECT country\n",
    "            FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "            WHERE pollutant != 'ppm'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_ppm = open_aq.query_to_pandas_safe(query, max_gb_scanned=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_ppm['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_ppm['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which pollutants have a value of exactly 0?\n",
    "\n",
    "query = (\"\"\"SELECT DISTINCT(pollutant)\n",
    "            FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "            WHERE value = 0 \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppm_zero = open_aq.query_to_pandas_safe(query, max_gb_scanned=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppm_zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Scavenger Hunt Day 2 - GROUP BY... HAVING and COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a helper object for our bigquery dataset\n",
    "hacker_news = bq_helper.BigQueryHelper(active_project= \"bigquery-public-data\", \n",
    "                                       dataset_name = \"hacker_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hacker_news.head('comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT parent, COUNT(id)\n",
    "           FROM `bigquery-public-data.hacker_news.comments`\n",
    "           GROUP BY parent\n",
    "           HAVING COUNT(id) > 200\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = hacker_news.query_to_pandas_safe(query, max_gb_scanned=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the questions I would like you to get the data to answer:\n",
    "\n",
    "* How many stories (use the \"id\" column) are there of each type (in the \"type\" column) in the full table?\n",
    "* How many comments have been deleted? (If a comment was deleted the \"deleted\" column in the comments table will have the value \"True\".)\n",
    "* Optional extra credit: read about aggregate functions other than COUNT() and modify one of the queries you wrote above to use a different aggregate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a list of all the tables in the hacker_news dataset\n",
    "hacker_news.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hacker_news.head('full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hacker_news['full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many stories (use the \"id\" column) are there of each type (in the \"type\" column) in the full table?\n",
    "\n",
    "query = \"\"\"SELECT type, COUNT(id)\n",
    "           FROM `bigquery-public-data.hacker_news.full`\n",
    "           GROUP BY type\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_by_type = hacker_news.query_to_pandas_safe(query, max_gb_scanned=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many comments have been deleted? (If a comment was deleted the \"deleted\" column in the comments table will have the value \"True\".)\n",
    "\n",
    "query = \"\"\"SELECT deleted, COUNT(id)\n",
    "           FROM `bigquery-public-data.hacker_news.full`\n",
    "           WHERE deleted = True\n",
    "           GROUP BY deleted\n",
    "           \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted = hacker_news.query_to_pandas_safe(query, max_gb_scanned=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Scavenger Hunt Day 3 - ORDER BY and DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a helper object for this dataset\n",
    "accidents = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",\n",
    "                                   dataset_name=\"nhtsa_traffic_fatalities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a list of all the tables in the accidents dataset\n",
    "accidents.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print information on all the columns in the \"accident_2015\" table\n",
    "# in the accidents dataset\n",
    "accidents.table_schema(\"accident_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to look at which day of the week the most fatal traffic accidents happen on\n",
    "# in 2015\n",
    "\n",
    "query = \"\"\"SELECT COUNT(consecutive_number),\n",
    "           EXTRACT(DAYOFWEEK FROM timestamp_of_crash)\n",
    "           FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n",
    "           GROUP BY EXTRACT(DAYOFWEEK FROM timestamp_of_crash)\n",
    "           ORDER BY COUNT(consecutive_number) DESC\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_most_fatal_2015 = accidents.query_to_pandas_safe(query, max_gb_scanned=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_most_fatal_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 - Sunday\n",
    "* 2 - Monday\n",
    "* 3 - Tuesday\n",
    "* 4 - Wednesday\n",
    "* 5 - Thursday\n",
    "* 6 - Friday\n",
    "* 7 - Saturday\n",
    "\n",
    "The most fatal accidents in 2015 occurred on Saturday and the fewest occurred on Tuesday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make a plot to show that our data is, actually, sorted:\n",
    "plt.plot(day_most_fatal_2015.f0_)\n",
    "plt.title(\"Number of Accidents by Rank of Day \\n (Most to least dangerous)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print information on all the columns in the \"accident_2016\" table\n",
    "# in the accidents dataset\n",
    "accidents.table_schema(\"accident_2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to look at which day of the week the most fatal traffic accidents happen on in 2016\n",
    "\n",
    "query = \"\"\"SELECT COUNT(consecutive_number),\n",
    "           EXTRACT(DAYOFWEEK FROM timestamp_of_crash)\n",
    "           FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2016`\n",
    "           GROUP BY EXTRACT(DAYOFWEEK FROM timestamp_of_crash)\n",
    "           ORDER BY COUNT(consecutive_number) DESC\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_most_fatal_2016 = accidents.query_to_pandas_safe(query, max_gb_scanned=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_most_fatal_2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most fatal accidents in 2016 occurred on Saturday and the fewest occurred on Tuesday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make a plot to show that our data is, actually, sorted:\n",
    "plt.plot(day_most_fatal_2016.f0_)\n",
    "plt.title(\"Number of Accidents by Rank of Day \\n (Most to least dangerous)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the questions I would like you to get the data to answer:\n",
    "\n",
    "* Which hours of the day do the most accidents occur during? <br/>\n",
    "<br/>\n",
    "** Return a table that has information on how many accidents occurred in each hour of the day in 2015, sorted by the the number of accidents which occurred each hour. Use either the accident_2015 or accident_2016 table for this, and the timestamp_of_crash column. (Yes, there is an hour_of_crash column, but if you use that one you won't get a chance to practice with dates. :P)\n",
    "Hint: You will probably want to use the EXTRACT() function for this. **\n",
    "\n",
    "* Which state has the most hit and runs? <br/>\n",
    "<br/>\n",
    "** Return a table with the number of vehicles registered in each state that were involved in hit-and-run accidents, sorted by the number of hit and runs. Use either the vehicle_2015 or vehicle_2016 table for this, especially the registration_state_name and hit_and_run columns. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which hours of the day do the most accidents occur during?\n",
    "\n",
    "query = \"\"\"SELECT COUNT(consecutive_number),\n",
    "                  EXTRACT(HOUR FROM timestamp_of_crash)\n",
    "           FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n",
    "           GROUP BY EXTRACT(HOUR FROM timestamp_of_crash)\n",
    "           ORDER BY 1 DESC\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_hours_2015 = accidents.query_to_pandas_safe(query, max_gb_scanned=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_hours_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most fatal accidents occur during the 6 pm hour. The least during the 4 AM hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents.head('vehicle_2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which state has the most hit and runs?\n",
    "# registration_state_name and hit_and_run columns\n",
    "\n",
    "\n",
    "query = \"\"\"SELECT COUNT(hit_and_run),\n",
    "                  registration_state_name\n",
    "            FROM `bigquery-public-data.nhtsa_traffic_fatalities.vehicle_2015`\n",
    "            WHERE hit_and_run = 'Yes'\n",
    "            GROUP BY registration_state_name\n",
    "            ORDER BY 1 DESC                  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_run_by_state = accidents.query_to_pandas_safe(query, max_gb_scanned=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_run_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
